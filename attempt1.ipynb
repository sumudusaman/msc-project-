{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1\n",
    "My first attempt was to create a simple model using LSTM to understand how the LSTM works, what should be the input, how to measure the accuracy if the model. For the first attempt I did not try to use the actual dataset taken from the contact center. Instead, I used a imdb review data set.  \n",
    "\n",
    "Machine learning and deep learning algorithms does not deal with text directly. So the text must be first transformed into a format which is understood my model. This approach of transforming text into number vector is called word embedding. While there are many ways of achieving this, for the first attempts I chose to use the facility provided by keras. Keras provides the libraries which can be used for text pre-processing. First I used the Tokenizer which converts words into numbers.\n",
    "\n",
    "Below is a sample code which demonstrates how Tokenizer is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique token count: 11\n",
      "\n",
      "Word index:  {'the': 1, 'fox': 2, 'lazy': 3, 'dog': 4, 'quick': 5, 'brown': 6, 'jumps': 7, 'over': 8, 'is': 9, 'not': 10, 'as': 11}\n",
      "\n",
      "Sequences:  [[1, 5, 6, 2, 7, 8, 1, 3, 4], [1, 2, 9, 10, 3, 11, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "inputsample = ['The quick brown fox jumps over the lazy dog.', 'The fox is not lazy as the dog.']\n",
    "\n",
    "# create a tokenizer limiting the maximum number of words to 1000\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "\n",
    "# update the vocabulary based in input samples\n",
    "tokenizer.fit_on_texts(inputsample)\n",
    "\n",
    "# Transforms each text in texts to a sequence of integers\n",
    "isequences = tokenizer.texts_to_sequences(inputsample)\n",
    "\n",
    "# Print the results\n",
    "word_index = tokenizer.word_index\n",
    "print('Unique token count: %s' % len(word_index))\n",
    "print('\\nWord index: ', word_index)\n",
    "print('\\nSequences: ', isequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, each unique word in the sentence was assigned with an integer. Word index above represents the words using integers. And the sequences shows the number vector representation for input text based sentences.\n",
    "When we use the tokenizer to generate number vectors from words, it generates high dimensional vectors because it is a sparse array. This is because the vector is in the mode of binary. If the word is present it assigns 1, else 0. So there can be many 0s in each sentence.\n",
    "To avoid generating such high dimensional vectors, we can use word embedding technique. Word embedding uses dense array by compacting the items into lower dimension vectors.\n",
    "To generate work embeddings, either we can generate from our data set or we can use transfer learning techniques to use pre trained word embeddings.\n",
    "Keras provides a very good library to generate word embeddings. It is important to generate the word embedding related to the problem we are going to solve. Keras word embedding with backpropagation provides a rich library to generate word embeddings. In below sample code I try to demonstrate how word embedding layer is added to a model. I used the imdb review data set for demonstrations purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# limit the max unique word count to 10,000\n",
    "max_features = 10000\n",
    "# define the max length of a sentence to 20 words. The remaining words will be truncated\n",
    "maxlen = 20\n",
    "\n",
    "# load the imdb data set \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# all sentences should be equal in length. Use padding to make all sentences same length\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a train data set and a test data set loaded from imdb. Below code I will try to demonstrate how the word embedding layer is added. I build a Sequential model starting with a Embedding layer. And the final output is a Dense layer with sigmoid activation because here the classification is 'positive' or 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "import numpy as np\n",
    "\n",
    "# set the random seed to 3 so we keep it constant so the model is reproducable\n",
    "# (without this the result may vary and validation accuracy was low)\n",
    "np.random.seed(3)\n",
    "\n",
    "# create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add embedding layer with 10000 input dimention and output dimension as 8, and the max input length for each sentence as 20\n",
    "model.add(Embedding(input_dim=10000, output_dim = 8, input_length=maxlen))\n",
    "\n",
    "# after embedding, we have to flattern the embeddings into 2D shape before giving the input to next Dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# add a dense layer with a single output and sigmoid activation (since this is a binary classification porblem)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# now we can compile the model with rmsprop optimizer and binary_crossentropy as the loss. Here we select binary_crossentropy \n",
    "# because this is a binary classification problem\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "# print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train and validate the model using imdb data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 3s 125us/step - loss: 0.2844 - acc: 0.8849 - val_loss: 0.5355 - val_acc: 0.7482\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.2691 - acc: 0.8940 - val_loss: 0.5453 - val_acc: 0.7462\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s 132us/step - loss: 0.2542 - acc: 0.9005 - val_loss: 0.5579 - val_acc: 0.7446\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 128us/step - loss: 0.2403 - acc: 0.9082 - val_loss: 0.5687 - val_acc: 0.7400\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 0.2267 - acc: 0.9142 - val_loss: 0.5828 - val_acc: 0.7392\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 0.2144 - acc: 0.9201 - val_loss: 0.5968 - val_acc: 0.7368\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 2s 120us/step - loss: 0.2020 - acc: 0.9257 - val_loss: 0.6104 - val_acc: 0.7354\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 2s 116us/step - loss: 0.1907 - acc: 0.9305 - val_loss: 0.6264 - val_acc: 0.7320\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 2s 119us/step - loss: 0.1803 - acc: 0.9353 - val_loss: 0.6419 - val_acc: 0.7276\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 2s 111us/step - loss: 0.1704 - acc: 0.9387 - val_loss: 0.6576 - val_acc: 0.7250\n"
     ]
    }
   ],
   "source": [
    "# train and validate the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy is close to 75% in above example when only 20 words from each sentence is used. Using more words from a sentence will help to increase this accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above sample was done to understand how a Embedding layer works. Now we can jump into next level by using the same Embedding concept but for a multi class classifier. In this project, we intend to find a class (Intent) for an input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuters dataset from Keras is a good dataset which can be used for a multi class classification problem. I will be using reuters data so understand how word embedding and a sequential model works for a multi class classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes 46\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import reuters\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "(x_train_reuters, y_train_reuters), (x_test_reuters, y_test_reuters) = reuters.load_data(num_words=None, test_split=0.2)\n",
    "\n",
    "max_words = 10000\n",
    "\n",
    "num_classes = max(y_train_reuters) + 1\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train_reuters = tokenizer.sequences_to_matrix(x_train_reuters, mode='binary')\n",
    "x_test_reuters = tokenizer.sequences_to_matrix(x_test_reuters, mode='binary')\n",
    "\n",
    "y_train_reuters = tf.keras.utils.to_categorical(y_train_reuters, num_classes)\n",
    "y_test_reuters = tf.keras.utils.to_categorical(y_test_reuters, num_classes)\n",
    "\n",
    "print('Number of classes %s' % num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 46 different classes in reuters dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation,LSTM\n",
    "\n",
    "\n",
    "modelmc = Sequential()\n",
    "modelmc.add(Dense(512, input_shape=(max_words,)))\n",
    "modelmc.add(Activation('relu'))\n",
    "modelmc.add(Dropout(0.5))\n",
    "modelmc.add(Dense(num_classes))\n",
    "modelmc.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 512)               5120512   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 46)                23598     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 46)                0         \n",
      "=================================================================\n",
      "Total params: 5,144,110\n",
      "Trainable params: 5,144,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelmc.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(modelmc.metrics_names)\n",
    "modelmc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/10\n",
      "8083/8083 [==============================] - 12s 1ms/step - loss: 0.2184 - acc: 0.9483 - val_loss: 0.9687 - val_acc: 0.8020\n",
      "Epoch 2/10\n",
      "8083/8083 [==============================] - 12s 1ms/step - loss: 0.1986 - acc: 0.9546 - val_loss: 1.0316 - val_acc: 0.8009\n",
      "Epoch 3/10\n",
      "8083/8083 [==============================] - 12s 1ms/step - loss: 0.1879 - acc: 0.9571 - val_loss: 0.9969 - val_acc: 0.8131\n",
      "Epoch 4/10\n",
      "8083/8083 [==============================] - 12s 1ms/step - loss: 0.1803 - acc: 0.9581 - val_loss: 1.0598 - val_acc: 0.8020\n",
      "Epoch 5/10\n",
      "8083/8083 [==============================] - 11s 1ms/step - loss: 0.1803 - acc: 0.9565 - val_loss: 1.0948 - val_acc: 0.7920\n",
      "Epoch 6/10\n",
      "8083/8083 [==============================] - 12s 1ms/step - loss: 0.1777 - acc: 0.9571 - val_loss: 1.1117 - val_acc: 0.7920\n",
      "Epoch 7/10\n",
      "8083/8083 [==============================] - 12s 1ms/step - loss: 0.1754 - acc: 0.9593 - val_loss: 1.0743 - val_acc: 0.8020\n",
      "Epoch 8/10\n",
      "8083/8083 [==============================] - 12s 1ms/step - loss: 0.1717 - acc: 0.9587 - val_loss: 1.1144 - val_acc: 0.7953\n",
      "Epoch 9/10\n",
      "8083/8083 [==============================] - 12s 1ms/step - loss: 0.1644 - acc: 0.9597 - val_loss: 1.1385 - val_acc: 0.8065\n",
      "Epoch 10/10\n",
      "8083/8083 [==============================] - 12s 1ms/step - loss: 0.1648 - acc: 0.9615 - val_loss: 1.1635 - val_acc: 0.7953\n",
      "2246/2246 [==============================] - 1s 235us/step\n",
      "Test loss: 1.0783489140037543\n",
      "Test accuracy: 0.8018699911483568\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "historymc = modelmc.fit(x_train_reuters, y_train_reuters, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "score = modelmc.evaluate(x_test_reuters, y_test_reuters, batch_size=batch_size, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a test accuracy close to 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will try to modify the model by adding LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation,LSTM\n",
    "\n",
    "\n",
    "modellstm = Sequential()\n",
    "modellstm.add(Embedding(input_dim=10000, output_dim = 64))\n",
    "modellstm.add(LSTM(32))\n",
    "modellstm.add(Dense(64, input_shape=(max_words,)))\n",
    "modellstm.add(Activation('relu'))\n",
    "modellstm.add(Dropout(0.5))\n",
    "modellstm.add(Dense(num_classes))\n",
    "modellstm.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_28 (Embedding)     (None, None, 64)          640000    \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 46)                2990      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 46)                0         \n",
      "=================================================================\n",
      "Total params: 657,518\n",
      "Trainable params: 657,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modellstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "modellstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/10\n",
      "8083/8083 [==============================] - 2501s 309ms/step - loss: 2.7429 - acc: 0.3146 - val_loss: 2.4692 - val_acc: 0.3315\n",
      "Epoch 2/10\n",
      "8083/8083 [==============================] - 1401s 173ms/step - loss: 2.5018 - acc: 0.3403 - val_loss: 2.4691 - val_acc: 0.3315\n",
      "Epoch 3/10\n",
      "8083/8083 [==============================] - 1026s 127ms/step - loss: 2.4743 - acc: 0.3514 - val_loss: 2.4689 - val_acc: 0.3315\n",
      "Epoch 4/10\n",
      "8083/8083 [==============================] - 1027s 127ms/step - loss: 2.4652 - acc: 0.3531 - val_loss: 2.4666 - val_acc: 0.3315\n",
      "Epoch 5/10\n",
      "8083/8083 [==============================] - 1026s 127ms/step - loss: 2.4574 - acc: 0.3538 - val_loss: 2.4654 - val_acc: 0.3315\n",
      "Epoch 6/10\n",
      "8083/8083 [==============================] - 1025s 127ms/step - loss: 2.4533 - acc: 0.3540 - val_loss: 2.4725 - val_acc: 0.3315\n",
      "Epoch 7/10\n",
      "8083/8083 [==============================] - 1025s 127ms/step - loss: 2.4492 - acc: 0.3540 - val_loss: 2.4715 - val_acc: 0.3315\n",
      "Epoch 8/10\n",
      "8083/8083 [==============================] - 1024s 127ms/step - loss: 2.4498 - acc: 0.3540 - val_loss: 2.4672 - val_acc: 0.3315\n",
      "Epoch 9/10\n",
      "8083/8083 [==============================] - 1031s 128ms/step - loss: 2.4471 - acc: 0.3540 - val_loss: 2.4705 - val_acc: 0.3315\n",
      "Epoch 10/10\n",
      "8083/8083 [==============================] - 1033s 128ms/step - loss: 2.4370 - acc: 0.3540 - val_loss: 2.4678 - val_acc: 0.3315\n",
      "2246/2246 [==============================] - 37s 16ms/step\n",
      "Test loss: 2.419490618676027\n",
      "Test accuracy: 0.36197684778237277\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "historylstm = modellstm.fit(x_train_reuters, y_train_reuters, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "scorelstm = modellstm.evaluate(x_test_reuters, y_test_reuters, batch_size=batch_size, verbose=1)\n",
    "print('Test loss:', scorelstm[0])\n",
    "print('Test accuracy:', scorelstm[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding LSTM layer, the training process for wach epoch takes abount 60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total accurcy achived was close to 36% which is very low. Probably the way I used LSTM might be wrong. Now I should find out how to increase the accuracy with LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
